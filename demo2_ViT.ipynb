{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (embedding): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (to_cls_token): Identity()\n",
      "  (mlp_head): Sequential(\n",
      "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=768, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "cuda\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\G7\\anaconda3\\envs\\deeplearningGPU\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from patchify import patchify\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define a simple Vision Transformer model\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=256, patch_size=16, num_classes=10, dim=768, depth=6, heads=8, mlp_dim=768, device=None):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.device = device   ################################################################################# ADD A DEVICE PARAMETER HERE!!!!!!!!\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        assert img_size % patch_size == 0, 'Image size must be divisible by patch size'\n",
    "        num_patches = (img_size // patch_size) ** 2    #256\n",
    "        self.patch_dim = 3 * patch_size * patch_size  #768\n",
    "        self.num_patches = num_patches   #256\n",
    "\n",
    "        self.embedding = nn.Linear(self.patch_dim, dim)\n",
    "        self.position_embeddings = nn.Parameter(torch.randn(1, num_patches, dim))\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(dim, heads, mlp_dim, device = self.device),\n",
    "            num_layers=depth\n",
    "        )\n",
    "        self.to_cls_token = nn.Identity()\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert image to patches\n",
    "        # patches = x.unfold(3, self.patch_dim, self.patch_dim).unfold(3, self.patch_dim, self.patch_dim)\n",
    "        # patches = patches.contiguous().view(x.size(0), self.num_patches, -1)\n",
    "        \n",
    "        x = x.to(\"cpu\")\n",
    "        print (f'x is: {x.shape}')\n",
    "        image_np = x.permute(1, 2, 0).numpy()  # Convert to (H, W, C)  ## 256, 256, 3\n",
    "        print (image_np.shape)\n",
    "        patches = patchify(image_np, (self.patch_size, self.patch_size, image_np.shape[2]), step=self.patch_size)  #\n",
    "        print (patches.shape)\n",
    "        patches = patches.reshape(-1, self.patch_size, self.patch_size, image_np.shape[2])  # Flatten patches\n",
    "        print (patches.shape)\n",
    "        patches = torch.tensor(patches).permute(0, 3, 1, 2)  # Convert to (num_patches, C, patch_size, patch_size)\n",
    "        print (patches.shape)\n",
    "        patches = patches.reshape(-1, self.num_patches, self.patch_dim)  # Flatten patches\n",
    "        print (patches.shape)\n",
    "        patches = patches.to(device)\n",
    "        print (patches.shape)\n",
    "               \n",
    "        \n",
    "        \n",
    "        # Linear embedding\n",
    "        x = self.embedding(patches)\n",
    "        # Add position embeddings\n",
    "        x += self.position_embeddings\n",
    "        # Transformer encoding\n",
    "        print (f'tranformer input is: {x.device}')\n",
    "\n",
    "        x = self.transformer(x).to(self.device)\n",
    "        print (f'tranformer output is: {x.device}')\n",
    "        # Classification token\n",
    "        x = self.to_cls_token(x.mean(dim=1))\n",
    "        # MLP head\n",
    "        return self.mlp_head(x)\n",
    "\n",
    "# Model parameters\n",
    "img_size = 256\n",
    "patch_size = 16\n",
    "num_classes = 10\n",
    "dim = 768\n",
    "depth = 6\n",
    "heads = 8\n",
    "mlp_dim = 768\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Instantiate the model and move it to the GPU\n",
    "model = VisionTransformer(img_size, patch_size, num_classes, dim, depth, heads, mlp_dim, device=device)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n",
    "print (device)\n",
    "print (model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x is: torch.Size([3, 256, 256])\n",
      "(256, 256, 3)\n",
      "(16, 16, 1, 16, 16, 3)\n",
      "(256, 16, 16, 3)\n",
      "torch.Size([256, 3, 16, 16])\n",
      "torch.Size([1, 256, 768])\n",
      "torch.Size([1, 256, 768])\n",
      "tranformer input is: cuda:0\n",
      "tranformer output is: cuda:0\n",
      "Output shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# Create dummy input data (batch of images)\n",
    "# batch_size = 8\n",
    "img_size = 256\n",
    "patch_size = 16\n",
    "num_classes = 10\n",
    "dim = 768\n",
    "depth = 2\n",
    "heads = 8\n",
    "mlp_dim = 768\n",
    "\n",
    "model = VisionTransformer(img_size, patch_size, num_classes, dim, depth, heads, mlp_dim).to(device)\n",
    "\n",
    "T = torch.randn(3, img_size, img_size).to(device)     \n",
    "\n",
    "# Forward pass\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    output = model(T)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-summary\n",
      "  Downloading torch_summary-1.4.5-py3-none-any.whl.metadata (18 kB)\n",
      "Downloading torch_summary-1.4.5-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: torch-summary\n",
      "Successfully installed torch-summary-1.4.5\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x is: torch.Size([3, 256, 256])\n",
      "(256, 256, 3)\n",
      "(16, 16, 1, 16, 16, 3)\n",
      "(256, 16, 16, 3)\n",
      "torch.Size([256, 3, 16, 16])\n",
      "torch.Size([1, 256, 768])\n",
      "torch.Size([1, 256, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "VisionTransformer                             [1, 10]                   196,608\n",
       "├─Linear: 1-1                                 [1, 256, 768]             590,592\n",
       "├─TransformerEncoder: 1-2                     [1, 256, 768]             --\n",
       "│    └─ModuleList: 2-1                        --                        --\n",
       "│    │    └─TransformerEncoderLayer: 3-1      [1, 256, 768]             3,546,624\n",
       "│    │    └─TransformerEncoderLayer: 3-2      [1, 256, 768]             3,546,624\n",
       "│    │    └─TransformerEncoderLayer: 3-3      [1, 256, 768]             3,546,624\n",
       "│    │    └─TransformerEncoderLayer: 3-4      [1, 256, 768]             3,546,624\n",
       "│    │    └─TransformerEncoderLayer: 3-5      [1, 256, 768]             3,546,624\n",
       "│    │    └─TransformerEncoderLayer: 3-6      [1, 256, 768]             3,546,624\n",
       "├─Identity: 1-3                               [1, 768]                  --\n",
       "├─Sequential: 1-4                             [1, 10]                   --\n",
       "│    └─LayerNorm: 2-2                         [1, 768]                  1,536\n",
       "│    └─Linear: 2-3                            [1, 10]                   7,690\n",
       "===============================================================================================\n",
       "Total params: 22,076,170\n",
       "Trainable params: 22,076,170\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 7.71\n",
       "===============================================================================================\n",
       "Input size (MB): 0.79\n",
       "Forward/backward pass size (MB): 39.33\n",
       "Params size (MB): 30.82\n",
       "Estimated Total Size (MB): 70.94\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "img_size = 256\n",
    "patch_size = 16\n",
    "num_classes = 10\n",
    "dim = 768\n",
    "depth = 6\n",
    "heads = 8\n",
    "mlp_dim = 768\n",
    "\n",
    "model = VisionTransformer(img_size, patch_size, num_classes, dim, depth, heads, mlp_dim).to(device)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "input_size = 3, 256, 256\n",
    "summary(model, input_size=input_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearningGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
